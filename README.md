TODO:
1. Fine-tune a huggingface transformer using a huggingface text dataset.
2. Write a similar sized LLM from scratch in Pytorch, train it on the huggingface dataset.
3. Use Flash-Attention. Compare performance to regular attention.
4. Quantize the model. Compare performance before and after.
5. Find a dataset with malayalam-english code switched.
6. Fine tune the base model on the malayalam-english code switched model.